<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yichi Zhang </title> <meta name="author" content="Yichi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/soccer_icon.png?fac9a39e25e42105064d3d52455e4b82"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaobenyun.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%7A%68%61%6E%67%79%69%63@%75%6D%69%63%68.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=xkBBhY8AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/46868553" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/594zyc" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yichi-zhang-354a83128" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/594zyc" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yichi</span> Zhang </h1> <p class="desc"><b>Ph.D. Candidate</b> âŽŸ <b>UMich CSE</b> âŽŸ <b>Conversational AI</b> âŽŸ <b>Embodied AI</b> âŽŸ <b>Multi-Modal</b></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/UMich_Yichi-480.webp 480w,/assets/img/UMich_Yichi-800.webp 800w,/assets/img/UMich_Yichi-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/UMich_Yichi.jpg?948f5251b5e4b258cb8fa816cedea401" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="UMich_Yichi.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hello! My name is Yichi Zhang, and I am a Ph.D. candidate in Computer Science and Engineering at University of Michigan. I advised by Professor <a href="https://web.eecs.umich.edu/~chaijy/" rel="external nofollow noopener" target="_blank">Joyce Chai</a> as a member of the <a href="https://sled.eecs.umich.edu/" rel="external nofollow noopener" target="_blank">SLED lab</a>. I am broadly interested in the intersection between conversational and embodied AI research, with a particular focus on language grounding to visual and physical contextsï¼Œ multi-modal dialog, and 3D embodied decision making. I have won the <a href="https://www.amazon.science/alexa-prize/simbot-challenge" rel="external nofollow noopener" target="_blank">1st Amazon Alexa Prize SimBot Challenge</a> in 2023 as the team leader of <a href="https://www.amazon.science/alexa-prize/university-of-michigans-seagull-wins-alexa-prize-simbot-challenge" rel="external nofollow noopener" target="_blank">SEAGULL</a>.</p> <p>Before joining UMich, I obtained my Masterâ€™s in Information and Communication Engineering at Tsinghua University in 2020, advised by Professor <a href="http://oa.ee.tsinghua.edu.cn/ouzhijian/" rel="external nofollow noopener" target="_blank">Zhijian Ou</a>. In 2019, I worked with Professor <a href="https://www.cs.columbia.edu/~zhouyu/" rel="external nofollow noopener" target="_blank">Zhou Yu</a> as a visiting scholar on task-oriented dialog systems. I got my Bachelorâ€™s in Electronic Information Science and Technology from Tsinghua University in 2017.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 17, 2024</th> <td> <a href="https://github.com/Berkeley-NLP/Agent-Eval-Refine" rel="external nofollow noopener" target="_blank">Agent-Eval-Refine</a> won the best paper award at CVPRâ€™24 MAR Workshop, and is accepted to COLM 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 26, 2024</th> <td> <a href="https://groundhog-mllm.github.io/" rel="external nofollow noopener" target="_blank">GROUNDHOG</a> is accepted to CVPR 2024! see you in Seattle! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 15, 2024</th> <td> I will join <a href="https://research.facebook.com/" rel="external nofollow noopener" target="_blank">Meta Reality Labs</a> to work with <a href="https://shanemoon.com/" rel="external nofollow noopener" target="_blank">Dr. Shane Moon</a> in Summer 2024 as a Research Scientist Intern. Hope to meet new friends in Seattle! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 07, 2023</th> <td> Two papers accepted to EMNLP 2023! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 07, 2023</th> <td> We won the First Place ($500,000) in the <a href="https://www.amazon.science/alexa-prize/university-of-michigans-seagull-wins-alexa-prize-simbot-challenge" rel="external nofollow noopener" target="_blank">1st Amazon Alexa Prize SimBot Challenge</a>! It was an absolute honor to co-lead the amazing <a href="https://www.amazon.science/alexa-prize/teams/seagull-2022" rel="external nofollow noopener" target="_blank">Team SEAGULL</a> with <a href="https://jedyang.com/" rel="external nofollow noopener" target="_blank">Jed</a>! Big congrats to all of our team members! ðŸŽ‰ Read our technical report <a href="https://www.amazon.science/alexa-prize/proceedings/seagull-an-embodied-agent-for-instruction-following-through-situated-dialog" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">CVPR</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cvpr24_groundhog-480.webp 480w,/assets/img/publication_preview/cvpr24_groundhog-800.webp 800w,/assets/img/publication_preview/cvpr24_groundhog-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/cvpr24_groundhog.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cvpr24_groundhog.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang2024groundhog" class="col-sm-7"> <div class="title">GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</div> <div class="author"> <em>Yichi Zhang</em>,Â Ziqiao Ma ,Â Xiaofeng Gao ,Â Suhaila Shakiah ,Â Qiaozi Gao ,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2402.16846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/cvpr24_groundhog.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="btn btn-sm z-depth-0" role="button">Code (coming soon)</a> <a href="https://groundhog-mllm.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024groundhog</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GROUNDHOG: Grounding Large Language Models to Holistic Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Ma, Ziqiao and Gao, Xiaofeng and Shakiah, Suhaila and Gao, Qiaozi and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1234--1254}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">COLM</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/colm24-480.webp 480w,/assets/img/publication_preview/colm24-800.webp 800w,/assets/img/publication_preview/colm24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/colm24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="colm24.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="pan2024autonomous" class="col-sm-7"> <div class="title">Autonomous evaluation and refinement of digital agents</div> <div class="author"> Jiayi Pan ,Â <em>Yichi Zhang</em>,Â Nicholas Tomlin ,Â Yifei Zhou ,Â Sergey Levine ,Â andÂ Alane Suhr </div> <div class="periodical"> <em>In First Conference on Language Modeling</em> , Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2404.06474" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/colm24.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/Berkeley-NLP/Agent-Eval-Refine" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan2024autonomous</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Autonomous evaluation and refinement of digital agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Jiayi and Zhang, Yichi and Tomlin, Nicholas and Zhou, Yifei and Levine, Sergey and Suhr, Alane}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{First Conference on Language Modeling}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EMNLP</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp23_illusion-480.webp 480w,/assets/img/publication_preview/emnlp23_illusion-800.webp 800w,/assets/img/publication_preview/emnlp23_illusion-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/emnlp23_illusion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp23_illusion.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang-etal-2023-grounding" class="col-sm-7"> <div class="title">Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?</div> <div class="author"> <em>Yichi Zhang</em>,Â Jiayi Pan ,Â Yuchen Zhou ,Â Rui Pan ,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em> , Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2311.00047" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emnlp23_illusion.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://aclanthology.org/2023.emnlp-main.348.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/vl-illusion/GVIL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vl-illusion.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, humanâ€™s perception of reality isnâ€™t always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at [github.com/vl-illusion/dataset](https://github.com/vl-illusion/dataset).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2023-grounding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Pan, Jiayi and Zhou, Yuchen and Pan, Rui and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bouamor, Houda and Pino, Juan and Bali, Kalika}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.348}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.emnlp-main.348}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5718--5728}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EMNLP Findings</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp23_wtag-480.webp 480w,/assets/img/publication_preview/emnlp23_wtag-800.webp 800w,/assets/img/publication_preview/emnlp23_wtag-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/emnlp23_wtag.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp23_wtag.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="bao-etal-2023-foundation" class="col-sm-7"> <div class="title">Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?</div> <div class="author"> Yuwei Bao ,Â Keunwoo Yu ,Â <em>Yichi Zhang</em>,Â Shane Storks ,Â Itamar Bar-Yossef ,Â Alex Iglesia ,Â Megan Su ,Â Xiao Zheng ,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2023</em> , Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2311.00738" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emnlp23_wtag.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/sled-group/Watch-Talk-and-Guide" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our benchmark and baselines will provide a stepping stone for future work on situated task guidance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bao-etal-2023-foundation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bao, Yuwei and Yu, Keunwoo and Zhang, Yichi and Storks, Shane and Bar-Yossef, Itamar and de la Iglesia, Alex and Su, Megan and Zheng, Xiao and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bouamor, Houda and Pino, Juan and Bali, Kalika}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: EMNLP 2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-emnlp.824}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.findings-emnlp.824}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12325--12341}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EMNLP</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp22_danli-480.webp 480w,/assets/img/publication_preview/emnlp22_danli-800.webp 800w,/assets/img/publication_preview/emnlp22_danli-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/emnlp22_danli.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp22_danli.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang-etal-2022-danli" class="col-sm-7"> <div class="title">DANLI: Deliberative Agent for Following Natural Language Instructions</div> <div class="author"> <em>Yichi Zhang</em>,Â Jianing Yang ,Â Jiayi Pan ,Â Shane Storks ,Â Nikhil Devraj ,Â Ziqiao Ma ,Â Keunwoo Yu ,Â Yuwei Bao ,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em> , Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2210.12485" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emnlp22_danli.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=WrHynGmCzZE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/sled-group/DANLI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Recent years have seen an increasing amount of work on embodied AI agents that can perform tasks by following human language instructions. However, most of these agents are reactive, meaning that they simply learn and imitate behaviors encountered in the training data. These reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from past experience (e.g., natural language and egocentric vision). We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark. Moreover, the underlying reasoning and planning processes, together with our modular framework, offer impressive transparency and explainability to the behaviors of the agent. This enables an in-depth understanding of the agentâ€™s capabilities, which shed light on challenges and opportunities for future embodied agents for instruction following. The code is available at \urlhttps://github.com/sled-group/DANLI.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2022-danli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{DANLI}: Deliberative Agent for Following Natural Language Instructions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Yang, Jianing and Pan, Jiayi and Storks, Shane and Devraj, Nikhil and Ma, Ziqiao and Yu, Keunwoo and Bao, Yuwei and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.83}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.emnlp-main.83}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1280--1298}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">ACL Findings</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/acl21_hitut-480.webp 480w,/assets/img/publication_preview/acl21_hitut-800.webp 800w,/assets/img/publication_preview/acl21_hitut-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/acl21_hitut.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="acl21_hitut.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang2021hierarchical" class="col-sm-7"> <div class="title">Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring</div> <div class="author"> <em>Yichi Zhang</em>,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2106.03427" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/acl21_hitut.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/594zyc/HiTUT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-to-end architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2021hierarchical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4202--4213}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EACL</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eacl21_ardm-480.webp 480w,/assets/img/publication_preview/eacl21_ardm-800.webp 800w,/assets/img/publication_preview/eacl21_ardm-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/eacl21_ardm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eacl21_ardm.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="wu-etal-2021-alternating" class="col-sm-7"> <div class="title">Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models</div> <div class="author"> Qingyang Wu ,Â <em>Yichi Zhang</em>,Â Yu Li ,Â andÂ Zhou Yu </div> <div class="periodical"> <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em> , Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1910.03756" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/eacl21_ardm.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/qywu/ARDM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu-etal-2021-alternating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Qingyang and Zhang, Yichi and Li, Yu and Yu, Zhou}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.eacl-main.110}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.eacl-main.110}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1292--1301}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EMNLP Findings</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp21_trip-480.webp 480w,/assets/img/publication_preview/emnlp21_trip-800.webp 800w,/assets/img/publication_preview/emnlp21_trip-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/emnlp21_trip.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp21_trip.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="storks-etal-2021-tiered-reasoning" class="col-sm-7"> <div class="title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding</div> <div class="author"> Shane Storks ,Â Qiaozi Gao ,Â <em>Yichi Zhang</em>,Â andÂ Joyce Chai </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2021</em> , Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2109.04947" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emnlp21_trip.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/sled-group/verifiable-coherent-nlu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machinesâ€™ true ability in language understanding and reasoning. In this paper, we highlight the importance of evaluating the underlying reasoning process in addition to end performance. Toward this goal, we introduce Tiered Reasoning for Intuitive Physics (TRIP), a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machinesâ€™ reasoning process. Our empirical results show that while large LMs can achieve high end performance, they struggle to support their predictions with valid supporting evidence. The TRIP dataset and our baseline results will motivate verifiable evaluation of commonsense reasoning and facilitate future research toward developing better language understanding and reasoning models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">storks-etal-2021-tiered-reasoning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Storks, Shane and Gao, Qiaozi and Zhang, Yichi and Chai, Joyce}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: EMNLP 2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Punta Cana, Dominican Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-emnlp.422}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.findings-emnlp.422}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4902--4918}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">Applied Sciences</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecrf-480.webp 480w,/assets/img/publication_preview/ecrf-800.webp 800w,/assets/img/publication_preview/ecrf-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ecrf.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecrf.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="dai2021elastic" class="col-sm-7"> <div class="title">Elastic CRFs for Open-Ontology Slot Filling</div> <div class="author"> Yinpei Dai ,Â <em>Yichi Zhang</em>,Â Hong Liu ,Â Zhijian Ou ,Â Yi Huang ,Â andÂ Junlan Feng </div> <div class="periodical"> <em>Applied Sciences</em>, Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1811.01331" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ecrf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Slot filling is a crucial component in task-oriented dialog systems that is used to parse (user) utterances into semantic concepts called slots. An ontology is defined by the collection of slots and the values that each slot can take. The most widely used practice of treating slot filling as a sequence labeling task suffers from two main drawbacks. First, the ontology is usually pre-defined and fixed and therefore is not able to detect new labels for unseen slots. Second, the one-hot encoding of slot labels ignores the correlations between slots with similar semantics, which makes it difficult to share knowledge learned across different domains. To address these problems, we propose a new model called elastic conditional random field (eCRF), where each slot is represented by the embedding of its natural language description and modeled by a CRF layer. New slot values can be detected by eCRF whenever a language description is available for the slot. In our experiment, we show that eCRFs outperform existing models in both in-domain and cross-domain tasks, especially in predicting unseen slots and values.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dai2021elastic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Elastic CRFs for Open-Ontology Slot Filling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dai, Yinpei and Zhang, Yichi and Liu, Hong and Ou, Zhijian and Huang, Yi and Feng, Junlan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10675}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">AAAI</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/aaai20_fig-480.webp 480w,/assets/img/publication_preview/aaai20_fig-800.webp 800w,/assets/img/publication_preview/aaai20_fig-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/aaai20_fig.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aaai20_fig.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang2020task" class="col-sm-7"> <div class="title">Task-oriented dialog systems that consider multiple appropriate responses under the same context</div> <div class="author"> <em>Yichi Zhang</em>,Â Zhijian Ou ,Â andÂ Zhou Yu </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , Jan 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1911.10484" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/aaai20_damd.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/thu-spmi/damd-multiwoz" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Conversations have an intrinsic one-to-many property, which means that multiple responses can be appropriate for the same dialog context. In task-oriented dialogs, this property leads to different valid dialog policies towards task completion. However, none of the existing task-oriented dialog generation approaches takes this property into account. We propose a Multi-Action Data Augmentation (MADA) framework to utilize the one-to-many property to generate diverse appropriate dialog responses. Specifically, we first use dialog states to summarize the dialog history, and then discover all possible mappings from every dialog state to its different valid system actions. During dialog system training, we enable the current dialog state to map to all valid system actions discovered in the previous process to create additional state-action pairs. By incorporating these additional pairs, the dialog policy learns a balanced action distribution, which further guides the dialog model to generate diverse responses. Experimental results show that the proposed framework consistently improves dialog policy diversity, and results in improved response diversity and appropriateness. Our model obtains state-of-the-art results on MultiWOZ.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020task</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task-oriented dialog systems that consider multiple appropriate responses under the same context}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Ou, Zhijian and Yu, Zhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{05}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9604--9611}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">ACL</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/acl20_parg-480.webp 480w,/assets/img/publication_preview/acl20_parg-800.webp 800w,/assets/img/publication_preview/acl20_parg-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/acl20_parg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="acl20_parg.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="gao-etal-2020-paraphrase" class="col-sm-7"> <div class="title">Paraphrase Augmented Task-Oriented Dialog Generation</div> <div class="author"> Silin Gao ,Â <em>Yichi Zhang</em>,Â Zhijian Ou ,Â andÂ Zhou Yu </div> <div class="periodical"> <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em> , Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2004.07462" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/acl20_parg.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://slideslive.com/38928976/paraphrase-augmented-taskoriented-dialog-generation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Silin159/PARG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao-etal-2020-paraphrase</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Paraphrase Augmented Task-Oriented Dialog Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Silin and Zhang, Yichi and Ou, Zhijian and Yu, Zhou}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.60}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.acl-main.60}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{639--649}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">EMNLP</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp20_labes-480.webp 480w,/assets/img/publication_preview/emnlp20_labes-800.webp 800w,/assets/img/publication_preview/emnlp20_labes-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/emnlp20_labes.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp20_labes.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang-etal-2020-probabilistic" class="col-sm-7"> <div class="title">A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning</div> <div class="author"> <em>Yichi Zhang</em>,Â Zhijian Ou ,Â Min Hu ,Â andÂ Junlan Feng </div> <div class="periodical"> <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> , Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2009.08115" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emnlp20_labes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://slideslive.com/38939308/a-probabilistic-endtoend-taskoriented-dialog-model-with-latent-belief-states-towards-semisupervised-learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/thu-spmi/LABES" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2020-probabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Ou, Zhijian and Hu, Min and Feng, Junlan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.emnlp-main.740}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.emnlp-main.740}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9207--9219}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"> <abbr class="badge">INTERSPEECH</abbr> </div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/is2020_dasi-480.webp 480w,/assets/img/publication_preview/is2020_dasi-800.webp 800w,/assets/img/publication_preview/is2020_dasi-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/is2020_dasi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="is2020_dasi.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> <div id="zhang2020improved" class="col-sm-7"> <div class="title">Improved Learning of Word Embeddings with Word Definitions and Semantic Injection.</div> <div class="author"> <em>Yichi Zhang</em>,Â Yinpei Dai ,Â Zhijian Ou ,Â Huixin Wang ,Â andÂ Junlan Feng </div> <div class="periodical"> <em>In INTERSPEECH</em> , Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1811.01331" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/is2020_dasi.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://www.interspeech2020.org/index.php?m=content&amp;c=index&amp;a=show&amp;catid=354&amp;id=1174" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/thu-spmi/DASI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Recently, two categories of linguistic knowledge sources, word definitions from monolingual dictionaries and linguistic relations (e.g. synonymy and antonymy), have been leveraged separately to improve the traditional co-occurrence based methods for learning word embeddings. In this paper, we investigate to leverage these two kinds of resources together. Specifically, we propose a new method for word embedding specialization, named Definition Autoencoder with Semantic Injection (DASI). In our experiments, DASI outperforms its single-knowledge-source counterparts on two semantic similarity benchmarks, and the improvements are further justified on a downstream task of dialog state tracking. We also show that DASI is superior over simple combinations of existing methods in incorporating the two knowledge sources.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020improved</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improved Learning of Word Embeddings with Word Definitions and Semantic Injection.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yichi and Dai, Yinpei and Ou, Zhijian and Wang, Huixin and Feng, Junlan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{INTERSPEECH}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4253--4257}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div id="clustrmaps-widget" style="display: none;"> <a href="https://clustrmaps.com/site/1byye" title="Visit tracker" rel="external nofollow noopener" target="_blank"> <img src="//www.clustrmaps.com/map_v2.png?d=J3qYc0XWVrHUXGJ1a7jnVV9-IDSeGLj3s5EWyNyw4Cs&amp;cl=ffffff"> </a> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Yichi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>